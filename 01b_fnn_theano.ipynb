{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CONTENT\n",
    "\n",
    "* **a. FNN with 1-by-1 Stochastic** \n",
    "* **b. FNN with Batch Stochastic** \n",
    "===> SOLVING VANISHING GRADIENT PROBLEM <==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN with 1-by-1 Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "from theano import shared, function\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer1:\n",
    "    \n",
    "    def __init__(self, bShape, wShape, activation=sigmoid):\n",
    "        assert len(bShape) == len(wShape) == 2 # bShape, wShape: (row, col) 2-tuples.\n",
    "        self.b = shared(np.asarray(np.random.normal(loc=0.,scale=1.,size=bShape),\n",
    "                                   dtype=theano.config.floatX), borrow=True)\n",
    "        self.w = shared(np.asarray(np.random.normal(loc=0.,scale=np.sqrt(1./wShape[0]),size=wShape),\n",
    "                                   dtype=theano.config.floatX), borrow=True)\n",
    "        self.params = [self.b, self.w]\n",
    "        self.activation = activation\n",
    "    \n",
    "    def activate(self, x):\n",
    "        z = T.dot(self.w, x) + self.b\n",
    "        if self.activation is sigmoid: return self.activation(z)\n",
    "        elif self.activation is softmax: return self.activation(z.T[0])\n",
    "        else: return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NNTheano1:\n",
    "    \n",
    "    def __init__(self, sizes, lr=.01, momentum=.9, hiddenActivation=sigmoid, outputActivation=sigmoid):\n",
    "        self.numLayers = len(sizes)\n",
    "        assert self.numLayers >= 3 # must have at least 1 hidden layer.\n",
    "        self.layers = [ Layer1((nOut,1),(nOut,nIn),hiddenActivation) \n",
    "                        for nIn,nOut in zip(sizes[:-2],sizes[1:]) ]\n",
    "        self.layers.append(Layer1((sizes[-1],1),(sizes[-1],sizes[-2]),outputActivation))\n",
    "            # shapes of bias & weight matrices for layer L\n",
    "            #  b: nrow(L) * 1\n",
    "            #  w: nrow(L) * nrow(L-1)\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "        self.config(lr, momentum)\n",
    "    \n",
    "    def config(self, lr, momentum): \n",
    "        # create a symbolic graph for forward passing.\n",
    "        # lightening fast for evaluation.\n",
    "        x = T.matrix()\n",
    "        y = T.vector()\n",
    "        a = self.layers[0].activate(x)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            a = self.layers[i].activate(a)\n",
    "        cost = T.sum(np.nansum(-y*T.log(a) - (1.-y)*T.log(1-a)))\n",
    "        self.predict = function([x], a) # <======================== forward pass function.\n",
    "        self.cost = function([x, y], cost) # <===================== cost function.\n",
    "        # sgd config\n",
    "        updates = []\n",
    "        for param in self.params:\n",
    "            paramUpdate = shared(param.get_value()*0, broadcastable=param.broadcastable)\n",
    "            updates.append((param, param-lr*paramUpdate))\n",
    "            updates.append((paramUpdate, momentum*paramUpdate+(1.-momentum)*T.grad(cost,param)))\n",
    "        self.run = function([x, y], cost, updates=updates) # <===== sgd function.\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        counter = 0\n",
    "        for x,y in zip(X,Y):\n",
    "            self.run(x,y) \n",
    "            counter += 1\n",
    "            if counter % 5000 == 0: print \"Trained: %d data points\" % counter\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        correct = 0\n",
    "        for x,y in zip(X,Y):\n",
    "            correct += 1 if np.argmax(self.predict(x))==np.argmax(y) else 0\n",
    "        print \"Accuracy: %d / %d (%.2f%%)\" % (correct, len(X), (float(correct)/len(X))*100)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Toy Example: Logic Gate OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# E.G. OR\n",
    "'''\n",
    "x y | lor\n",
    "0 0 | 0\n",
    "0 1 | 1\n",
    "1 0 | 1\n",
    "1 1 | 1\n",
    "'''\n",
    "xy1, lor1 = np.asarray([0.,0.]).reshape(2,1), np.asarray([0.])\n",
    "xy2, lor2 = np.asarray([0.,1.]).reshape(2,1), np.asarray([1.])\n",
    "xy3, lor3 = np.asarray([1.,0.]).reshape(2,1), np.asarray([1.])\n",
    "xy4, lor4 = np.asarray([1.,1.]).reshape(2,1), np.asarray([1.])\n",
    "X = [xy1, xy2, xy3, xy4] * 12500\n",
    "Y = [lor1, lor2, lor3, lor4] * 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fnn: [2, 1, 1]\n",
    "'''\n",
    "fnn = NNTheano1([2,1,1],lr=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained: 5000 data points\n",
      "Trained: 10000 data points\n",
      "Trained: 15000 data points\n",
      "Trained: 20000 data points\n",
      "Trained: 25000 data points\n",
      "Trained: 30000 data points\n",
      "Trained: 35000 data points\n",
      "Trained: 40000 data points\n",
      "Trained: 45000 data points\n",
      "Trained: 50000 data points\n",
      "CPU times: user 1.92 s, sys: 45.7 ms, total: 1.97 s\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fnn.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Truth-Value Table\n",
      "0 0 | 0\n",
      "0 1 | 1\n",
      "1 0 | 1\n",
      "1 1 | 1\n"
     ]
    }
   ],
   "source": [
    "print \"OR Truth-Value Table\"\n",
    "print \"0 0 |\", \"0\" if fnn.predict(xy1)[0][0] < .5 else \"1\"\n",
    "print \"0 1 |\", \"0\" if fnn.predict(xy2)[0][0] < .5 else \"1\"\n",
    "print \"1 0 |\", \"0\" if fnn.predict(xy3)[0][0] < .5 else \"1\"\n",
    "print \"1 1 |\", \"0\" if fnn.predict(xy4)[0][0] < .5 else \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/NN/DATA/neural-networks-and-deep-learning-master/src\")\n",
    "import mnist_loader # works when under '~/src' path.\n",
    "data_train, data_dev, data_test = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [np.asarray(datum[0]).reshape(len(datum[0]),1) for datum in data_train]\n",
    "Y_train = [np.asarray(datum[1]).squeeze() for datum in data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = [np.asarray(datum[0]).reshape(len(datum[0]),1) for datum in data_test]\n",
    "Y_test = [np.asarray(datum[1]).squeeze() for datum in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn = NNTheano1([784, 30, 10], lr=.5, hiddenActivation=sigmoid, outputActivation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train cost:  3.70402235535\n"
     ]
    }
   ],
   "source": [
    "print \"Pre-train cost: \", fnn.cost(X_train[0],Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained: 5000 data points\n",
      "Trained: 10000 data points\n",
      "Trained: 15000 data points\n",
      "Trained: 20000 data points\n",
      "Trained: 25000 data points\n",
      "Trained: 30000 data points\n",
      "Trained: 35000 data points\n",
      "Trained: 40000 data points\n",
      "Trained: 45000 data points\n",
      "Trained: 50000 data points\n",
      "CPU times: user 11.7 s, sys: 75.2 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fnn.train(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-train cost:  0.00475332078753\n"
     ]
    }
   ],
   "source": [
    "print \"Post-train cost: \", fnn.cost(X_train[0],Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train Random Accuracy:  Accuracy: 35016 / 50000 (70.03%)\n",
      "None\n",
      "Pre-train Random Accuracy:  Accuracy: 1240 / 10000 (12.40%)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print \"Pre-train Random Accuracy: \", fnn.evaluate(X_train, Y_train)\n",
    "print \"Pre-train Random Accuracy: \", fnn.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# FNN with Batch Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "from theano import shared, function, scan\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer2:\n",
    "    \n",
    "    def __init__(self, bShape, wShape, activation=sigmoid):\n",
    "        assert len(bShape) == len(wShape) == 2 # bShape, wShape: (row, col) 2-tuples.\n",
    "        self.b = shared(np.asarray(np.random.normal(loc=0.,scale=1.,size=bShape),\n",
    "                                   dtype=theano.config.floatX), borrow=True)\n",
    "        self.w = shared(np.asarray(np.random.normal(loc=0.,scale=np.sqrt(1./wShape[0]),size=wShape),\n",
    "                                   dtype=theano.config.floatX), borrow=True)\n",
    "        self.params = [self.b, self.w]\n",
    "        self.activation = activation\n",
    "    \n",
    "    def activate(self, x):\n",
    "        z = T.dot(self.w, x) + self.b\n",
    "        if self.activation is sigmoid: return self.activation(z)\n",
    "        elif self.activation is softmax: return self.activation(z.T[0])\n",
    "        else: return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NNTheano2:\n",
    "    \n",
    "    def __init__(self, sizes, hiddenActivation=sigmoid, outputActivation=sigmoid):\n",
    "        self.numLayers = len(sizes)\n",
    "        assert self.numLayers >= 3 # must have at least 1 hidden layer.\n",
    "        self.layers = [ Layer1((nOut,1),(nOut,nIn),hiddenActivation) \n",
    "                        for nIn,nOut in zip(sizes[:-2],sizes[1:]) ]\n",
    "        self.layers.append(Layer1((sizes[-1],1),(sizes[-1],sizes[-2]),outputActivation))\n",
    "            # shapes of bias & weight matrices for layer L\n",
    "            #  b: nrow(L) * 1\n",
    "            #  w: nrow(L) * nrow(L-1)\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        a = self.layers[0].activate(x)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            a = self.layers[i].activate(a)\n",
    "        return a\n",
    "    \n",
    "    def sgd(self, lr=.01, momentum=.0):\n",
    "        # create symbolic graph for batch cost computation.\n",
    "        X = T.tensor3()\n",
    "        Y = T.matrix()\n",
    "        batchSize = T.iscalar()\n",
    "        A = scan(lambda x: self.predict(x), sequences=X)[0]\n",
    "        C = scan(lambda a,y: T.sum(np.nansum(-y*T.log(a) - (1.-y)*T.log(1-a))), sequences=[A,Y])[0]\n",
    "        cost = (1./batchSize) * T.sum(C)\n",
    "        updates = []\n",
    "        for param in self.params:\n",
    "            paramUpdate = shared(param.get_value()*0, broadcastable=param.broadcastable)\n",
    "            updates.append((param, param-lr*paramUpdate))\n",
    "            updates.append((paramUpdate, momentum*paramUpdate+(1.-momentum)*T.grad(cost,param)))\n",
    "        self.batch_update = function([X,Y,batchSize], cost, updates=updates)\n",
    "    \n",
    "    def train(self, data_train, epochs=1, batchSize=10):\n",
    "        assert len(data_train) == 2\n",
    "        X_train, Y_train = data_train\n",
    "        for i in xrange(epochs): \n",
    "            c = 0\n",
    "            batches = [ [X_train[k:k+batchSize],Y_train[k:k+batchSize]] \n",
    "                        for k in xrange(0,len(data_train),batchSize) ]\n",
    "            for batch in batches:\n",
    "                c = self.batch_update(np.array(batch[0]),np.array(batch[1]),len(batch[0]))\n",
    "            if i!=0 and i%10==0: print \"Epoch: %d | Cost: %.6f\" % (i,c)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Toy Example: Logic Gate OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E.G. OR\n",
    "'''\n",
    "x y | lor\n",
    "0 0 | 0\n",
    "0 1 | 1\n",
    "1 0 | 1\n",
    "1 1 | 1\n",
    "'''\n",
    "xy1, lor1 = np.asarray([0.,0.]).reshape(2,1), np.asarray([0.])\n",
    "xy2, lor2 = np.asarray([0.,1.]).reshape(2,1), np.asarray([1.])\n",
    "xy3, lor3 = np.asarray([1.,0.]).reshape(2,1), np.asarray([1.])\n",
    "xy4, lor4 = np.asarray([1.,1.]).reshape(2,1), np.asarray([1.])\n",
    "X = [xy1, xy2, xy3, xy4] * 12500\n",
    "Y = [lor1, lor2, lor3, lor4] * 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnn = NNTheano2([2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn.sgd(lr=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Cost: 0.548807\n",
      "Epoch: 20 | Cost: 0.325504\n",
      "Epoch: 30 | Cost: 0.173902\n",
      "Epoch: 40 | Cost: 0.103380\n",
      "Epoch: 50 | Cost: 0.070442\n",
      "Epoch: 60 | Cost: 0.052469\n",
      "Epoch: 70 | Cost: 0.041450\n",
      "Epoch: 80 | Cost: 0.034098\n",
      "Epoch: 90 | Cost: 0.028881\n"
     ]
    }
   ],
   "source": [
    "fnn.train([X,Y],epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Truth-Value Table\n",
      "0 0 | 0\n",
      "0 1 | 1\n",
      "1 0 | 1\n",
      "1 1 | 1\n"
     ]
    }
   ],
   "source": [
    "print \"OR Truth-Value Table\"\n",
    "print \"0 0 |\", \"0\" if fnn.predict(xy1).eval()[0][0] < .5 else \"1\"\n",
    "print \"0 1 |\", \"0\" if fnn.predict(xy2).eval()[0][0] < .5 else \"1\"\n",
    "print \"1 0 |\", \"0\" if fnn.predict(xy3).eval()[0][0] < .5 else \"1\"\n",
    "print \"1 1 |\", \"0\" if fnn.predict(xy4).eval()[0][0] < .5 else \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/NN/DATA/neural-networks-and-deep-learning-master/src\")\n",
    "import mnist_loader # works when under '~/src' path.\n",
    "data_train, data_dev, data_test = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [np.asarray(datum[0]).reshape(len(datum[0]),1) for datum in data_train]\n",
    "Y_train = [np.asarray(datum[1]).squeeze() for datum in data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = [np.asarray(datum[0]).reshape(len(datum[0]),1) for datum in data_test]\n",
    "Y_test = [np.asarray(datum[1]).squeeze() for datum in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn = NNTheano2([784, 30, 10], hiddenActivation=sigmoid, outputActivation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 64.3 ms, total: 10.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fnn.sgd(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Cost: 0.937225\n",
      "Epoch: 20 | Cost: 0.218448\n",
      "Epoch: 30 | Cost: 0.119370\n",
      "Epoch: 40 | Cost: 0.081350\n",
      "Epoch: 50 | Cost: 0.061352\n",
      "Epoch: 60 | Cost: 0.049066\n",
      "Epoch: 70 | Cost: 0.040775\n",
      "Epoch: 80 | Cost: 0.034817\n",
      "Epoch: 90 | Cost: 0.030334\n",
      "CPU times: user 133 ms, sys: 7.92 ms, total: 141 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fnn.train([X_train,Y_train],epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print sum(np.argmax(fnn.predict(X_train[i]).eval())==np.argmax(Y_train[i]) for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
